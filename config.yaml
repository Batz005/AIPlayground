# AIPlayground Configuration

modules:
  ingestor:
    simple:
      chunk_size: 100
    advanced:
      chunk_size: 150
      min_size: 50

  retriever:
    keyword:
      top_k: 5
    semantic:
      top_k: 3
      model_name: "sentence-transformers/paraphrase-MiniLM-L3-v2"

  generator:
    flan_t5_small:
      type: flan_t5_small
      max_new_tokens: 64
      temperature: 0.0
    flan_t5_base:
      type: flan_t5_base
      max_new_tokens: 64
      temperature: 0.0
    flan_alpaca_base:
      type: flan_alpaca_base
      max_new_tokens: 256
      temperature: 0.0
    # future: gpt, llama, etc.

pipelines:
  qa:
    sequence: ["ingestor:advanced", "retriever:semantic"]
  rag:
    sequence: ["ingestor:advanced", "retriever:semantic", "generator:flan_alpaca_base"]

embedding:
  model_name: "sentence-transformers/paraphrase-MiniLM-L3-v2"
  device: "cpu"

data:
  docs_path: "data/docs"
  chunks_cache_path: "cache/chunks.json"
  embeddings_cache_path: "cache/embeddings.pkl"

api:
  host: "127.0.0.1"
  port: 8000

logging:
  level: "INFO"